{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19FaRKY2wDY9K67vrqmc2PAMRedMrrNhX",
      "authorship_tag": "ABX9TyO2UN1Jrp4zhXSAzYNBi1A5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Car9777AZn2",
        "outputId": "b19a5304-39f8-4f41-bf4d-d11c8105bbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error index 67: cannot identify image file <_io.BytesIO object at 0x7aa038be2cf0>\n",
            "Error index 110: cannot identify image file <_io.BytesIO object at 0x7aa03d4527a0>\n",
            "Error index 523: cannot identify image file <_io.BytesIO object at 0x7aa03d503650>\n",
            "Error index 701: cannot identify image file <_io.BytesIO object at 0x7aa03d503240>\n",
            "Error index 860: cannot identify image file <_io.BytesIO object at 0x7aa03d5030b0>\n",
            "Error index 936: cannot identify image file <_io.BytesIO object at 0x7aa0754f13a0>\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/A2_Data.csv\")\n",
        "image_dataframe = pd.DataFrame(columns=[\"ProductID\", \"Image\", \"Review Text\"])\n",
        "\n",
        "for i, image_urls in enumerate(data[\"Image\"]):\n",
        "    try:\n",
        "        url_lst = image_urls.strip('][').split(', ')\n",
        "        rows = []\n",
        "        for url in url_lst:\n",
        "            url = url.strip(\"'\")\n",
        "            image = Image.open(requests.get(url, stream=True).raw)\n",
        "            row = {\n",
        "                'ProductID': data.index[i],\n",
        "                'Image': url,\n",
        "                'Review Text': data.loc[i, 'Review Text']\n",
        "            }\n",
        "            rows.append(row)\n",
        "        image_dataframe = pd.concat([image_dataframe, pd.DataFrame(rows)], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error index {i}: {e}\") #printing and skiping productID having invalid image url\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('image_dataframe(inital).pkl', 'wb') as f:\n",
        "    pickle.dump(image_dataframe, f) # dumping image_dataframe for further use"
      ],
      "metadata": {
        "id": "OMuKnRKdO4PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#transformation preprocessing the image (randomFlips,adjusting tone,resize and normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load pre-trained model(resnet 18)\n",
        "model = models.resnet18(pretrained = True)\n",
        "model.eval()\n",
        "\n",
        "# Function to preprocess the image, extract features, and normalize them\n",
        "def extract_features_from_image(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        image = image.convert(\"RGB\")\n",
        "        image_tensor = transform(image).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            features = model(image_tensor)  # Extract features using the model\n",
        "            features = torch.nn.functional.normalize(features, p=2, dim=1)  #Feature Normalization using pytorch\n",
        "            features = features.squeeze().view(-1).numpy()  # Flatten the vector\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "        return None\n",
        "\n",
        "# preprocessing and feature extraction of the dataset (commented part)\n",
        "#image_dataframe['Features'] = image_dataframe['Image'].apply(extract_features_from_image)\n"
      ],
      "metadata": {
        "id": "CgyiKFkWC0gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('image_features2.pkl', 'wb') as f:\n",
        "    pickle.dump(image_dataframe, f)# dump updated image_dataframe(with normalized image features)"
      ],
      "metadata": {
        "id": "yjLOja5dNNkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/image_dataframe(inital).pkl', 'rb') as file:\n",
        " image_dataframe_i = pickle.load(file)\n",
        "image_dataframe_i #inital image_dataframe with (image and review text coloumn)"
      ],
      "metadata": {
        "id": "LZDgoRSMSsYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import pickle\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# extract content between html tages from the dataframe\n",
        "def remove_tags(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for data in soup(['style', 'script']):\n",
        "        data.decompose()\n",
        "    return ' '.join(soup.stripped_strings)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = remove_tags(text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
        "    return tokens\n",
        "\n",
        "#image_dataframe (inital image_dataframe)\n",
        "text_data = image_dataframe_i['Review Text'].fillna('').tolist()\n",
        "\n",
        "# Preprocess text data\n",
        "tokenized_texts = [preprocess_text(text) for text in text_data]#######\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZnUt7Dx9owA",
        "outputId": "1423f4fd-a6e1-4f6e-8efe-284d3211688a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-9-c9dcbcd560f6>:25: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(html, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenized_texts.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenized_texts, f)"
      ],
      "metadata": {
        "id": "kpz9HzOkMNGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for tokenized texts\n",
        "df_tokenized_texts = pd.DataFrame({'Tokenized_Text': tokenized_texts})\n",
        "\n",
        "# Display the DataFrame\n",
        "df_tokenized_texts\n"
      ],
      "metadata": {
        "id": "SQ_VzQoNN-IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Create a DataFrame for tokenized texts\n",
        "df_tokenized_texts = pd.DataFrame({'Tokenized_Text': tokenized_texts})\n",
        "\n",
        "# Calculate Term Frequency (TF)\n",
        "def calculate_tf(df):\n",
        "    tf_scores = {}\n",
        "    for idx, row in df.iterrows():\n",
        "        word_counts = pd.Series(row['Tokenized_Text']).value_counts().to_dict()\n",
        "        total_words = sum(word_counts.values())\n",
        "        tf_scores[idx] = {word: count / total_words for word, count in word_counts.items()}\n",
        "    return tf_scores\n",
        "\n",
        "# Calculate Inverse Document Frequency (IDF)\n",
        "def calculate_idf(df):\n",
        "    document_frequency = {}\n",
        "    num_documents = len(df)\n",
        "\n",
        "    # Count how many documents contain each word\n",
        "    for idx, row in df.iterrows():\n",
        "        unique_words = set(row['Tokenized_Text'])\n",
        "        for word in unique_words:\n",
        "            document_frequency[word] = document_frequency.get(word, 0) + 1\n",
        "\n",
        "    # Calculate IDF scores\n",
        "    idf_scores = {word: math.log(num_documents / (freq + 1)) for word, freq in document_frequency.items()}\n",
        "    return idf_scores\n",
        "\n",
        "# Calculate TF-IDF scores\n",
        "def calculate_tf_idf(df, tf_scores, idf_scores):\n",
        "    tf_idf_scores = {}\n",
        "    for idx, row in df.iterrows():\n",
        "        tf_idf_scores[idx] = {}\n",
        "        for word in row['Tokenized_Text']:\n",
        "            tf_idf = tf_scores[idx].get(word, 0) * idf_scores.get(word, 0)\n",
        "            tf_idf_scores[idx][word] = tf_idf\n",
        "    return tf_idf_scores\n",
        "\n",
        "# Calculate TF scores\n",
        "tf_scores = calculate_tf(df_tokenized_texts)\n",
        "\n",
        "# Calculate IDF scores\n",
        "idf_scores = calculate_idf(df_tokenized_texts)\n",
        "\n",
        "# Calculate TF-IDF scores\n",
        "tf_idf_scores = calculate_tf_idf(df_tokenized_texts, tf_scores, idf_scores)\n",
        "\n",
        "# Convert TF-IDF scores to DataFrame\n",
        "tf_idf_df = pd.DataFrame(tf_idf_scores).fillna(0)\n",
        "tf_idf_df = tf_idf_df.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWRzxb9kREBg",
        "outputId": "ba5a5fa9-00f0-4020-836c-f83ecb822f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-83288f38ec89>:12: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  word_counts = pd.Series(row['Tokenized_Text']).value_counts().to_dict()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_df"
      ],
      "metadata": {
        "id": "RzyxpNbMXSsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tf-idf_feature.pkl', 'wb') as f:\n",
        "    pickle.dump(tf_idf_df, f)"
      ],
      "metadata": {
        "id": "U1AiG_hDY1qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/image_features2.pkl', 'rb') as file:\n",
        "  image_features = pickle.load(file)\n",
        "image_features"
      ],
      "metadata": {
        "id": "5llR8fzvG3lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/tf-idf_feature.pkl', 'rb') as file:\n",
        "  tf_idf = pickle.load(file)\n",
        "#tf_idf"
      ],
      "metadata": {
        "id": "X4l90BD9IHTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/tokenized_texts.pkl', 'rb') as file:\n",
        "  token = pickle.load(file)\n",
        "\n",
        "# Create a DataFrame for tokenized texts\n",
        "df_tokenized_texts = pd.DataFrame({'Tokenized_Text': token})\n",
        "\n",
        "# Display the DataFrame\n",
        "#df_tokenized_texts"
      ],
      "metadata": {
        "id": "cfcLtHeFUEbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "#cosine similarity function\n",
        "def cosine_similarity(v1, v2):\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    magnitude_v1 = np.linalg.norm(v1)\n",
        "    magnitude_v2 = np.linalg.norm(v2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if magnitude_v1 == 0 or magnitude_v2 == 0:\n",
        "        return 0\n",
        "\n",
        "    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n",
        "    return cosine_sim\n",
        "\n",
        "image_urls = []\n",
        "url_input = input(\"Enter image URL: \").strip() # input url\n",
        "review_input = input(\"Enter Text Review: \").strip() #input Text review\n",
        "doc1 = preprocess_text(review_input)\n",
        "\n",
        "# TF Calculation\n",
        "tf = [{word: doc1.count(word) / len(doc1) for word in doc1}]\n",
        "df = {}\n",
        "for doc in df_tokenized_texts['Tokenized_Text']:\n",
        "    for word in set(doc):\n",
        "        df[word] = df.get(word, 0) + 1\n",
        "\n",
        "# IDF Calculation (inverse document frequency)\n",
        "idf = {word: math.log(len(image_features) / freq) for word, freq in df.items()}\n",
        "\n",
        "#TF-IDF calculation\n",
        "tf_idf_doc1 = [{word: freq * idf.get(word, 0) for word, freq in doc.items()} for doc in tf]\n",
        "\n",
        "#TF calculation function\n",
        "def calculate_tf(df):\n",
        "    tf_scores = {}\n",
        "    for idx, row in df.iterrows():\n",
        "        word_counts = pd.Series(row['Tokenized_Text']).value_counts().to_dict()\n",
        "        total_words = sum(word_counts.values())\n",
        "        tf_scores[idx] = {word: count / total_words for word, count in word_counts.items()}\n",
        "    return tf_scores\n",
        "\n",
        "#IDF calculation function\n",
        "def calculate_idf(df):\n",
        "    document_frequency = {}\n",
        "    num_documents = len(df)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        unique_words = set(row['Tokenized_Text'])\n",
        "        for word in unique_words:\n",
        "            document_frequency[word] = document_frequency.get(word, 0) + 1\n",
        "\n",
        "    # Calculate IDF scores\n",
        "    idf_scores = {word: math.log(num_documents / (freq + 1)) for word, freq in document_frequency.items()}\n",
        "    return idf_scores\n",
        "\n",
        "#TF-IDF calculation\n",
        "def calculate_tf_idf(df, tf_scores, idf_scores):\n",
        "    tf_idf_scores = {}\n",
        "    for idx, row in df.iterrows():\n",
        "        tf_idf_scores[idx] = {}\n",
        "        for word in row['Tokenized_Text']:\n",
        "            tf_idf = tf_scores[idx].get(word, 0) * idf_scores.get(word, 0)\n",
        "            tf_idf_scores[idx][word] = tf_idf\n",
        "    return tf_idf_scores\n",
        "\n",
        "max_features = len(tf_idf.columns)\n",
        "for doc in tf_idf_doc1:\n",
        "    for word in list(doc.keys()):\n",
        "        if word not in tf_idf.columns:\n",
        "            del doc[word]\n",
        "    doc.update({word: 0 for word in tf_idf.columns if word not in doc})\n",
        "\n",
        "if url_input.startswith(\"[\") and url_input.endswith(\"]\"):\n",
        "    url_input = url_input[1:-1]\n",
        "    image_urls.extend([url.strip() for url in url_input.split(\",\")])\n",
        "elif url_input:\n",
        "\n",
        "    image_urls.append(url_input)\n",
        "\n",
        "#Images feature exactraction for input URL\n",
        "query_image_vectors = [extract_features_from_image(url) for url in image_urls]\n",
        "query_review_vector = pd.DataFrame(tf_idf_doc1, columns=tf_idf.columns)\n",
        "\n",
        "\n",
        "#Cosine similarities for images\n",
        "image_similarities = []\n",
        "for i, feature in enumerate(image_features['Features']):\n",
        "    similarities = []\n",
        "    for query_image_vector in query_image_vectors:\n",
        "        query_image_vector_reshaped = query_image_vector.reshape(1, -1)  # Reshape query_image_vector\n",
        "        cosine_sim_im = cosine_similarity(query_image_vector_reshaped, feature)\n",
        "        similarities.append(cosine_sim_im)\n",
        "\n",
        "    if i < len(tf_idf):\n",
        "        cosine_sim_rv = cosine_similarity(query_review_vector.iloc[0], tf_idf.iloc[i])\n",
        "    else:\n",
        "        cosine_sim_rv = 0\n",
        "\n",
        "    average_similarity = sum(similarities) / len(similarities)\n",
        "    composite_similarity = (cosine_sim_rv + average_similarity) / 2\n",
        "    image_similarities.append((i, average_similarity, cosine_sim_rv, composite_similarity))\n",
        "\n",
        "# Sorting in descending order\n",
        "image_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "review_similarities = image_similarities.copy()\n",
        "composite_similarities = image_similarities.copy()\n",
        "\n",
        "review_similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "composite_similarities.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "# Initialize a set to keep track of IDs\n",
        "processed_ids = set()\n",
        "\n",
        "#3 similar images\n",
        "top_similar_images = []\n",
        "for i, img_similarity, txt_similarity, composite_similarity in image_similarities:\n",
        "    if image_features['ProductID'][i] not in processed_ids:\n",
        "        top_similar_images.append((i, img_similarity, txt_similarity, composite_similarity))\n",
        "        processed_ids.add(image_features['ProductID'][i])\n",
        "    if len(top_similar_images) == 3:\n",
        "        break\n",
        "\n",
        "# 3 similar reviews Text\n",
        "top_similar_reviews = []\n",
        "for i, img_similarity, txt_similarity, composite_similarity in review_similarities:\n",
        "    if image_features['ProductID'][i] not in processed_ids:\n",
        "        top_similar_reviews.append((i, img_similarity, txt_similarity, composite_similarity))\n",
        "        processed_ids.add(image_features['ProductID'][i])\n",
        "    if len(top_similar_reviews) == 3:\n",
        "        break\n",
        "\n",
        "# 3 similar composite scores\n",
        "top_similar_composites = []\n",
        "for i, avg_similarity, txt_similarity, composite_similarity in composite_similarities:\n",
        "    if image_features['ProductID'][i] not in processed_ids:\n",
        "        top_similar_composites.append((i, avg_similarity, txt_similarity, composite_similarity))\n",
        "        processed_ids.add(image_features['ProductID'][i])\n",
        "    if len(top_similar_composites) == 3:\n",
        "        break\n",
        "\n",
        "# Save top 3 similar images and reviews\n",
        "with open('top_similar_images.pkl', 'wb') as f:\n",
        "    pickle.dump(top_similar_images, f)\n",
        "\n",
        "with open('top_similar_reviews.pkl', 'wb') as f:\n",
        "    pickle.dump(top_similar_reviews, f)\n",
        "\n",
        "with open('top_similar_composites.pkl', 'wb') as f:\n",
        "    pickle.dump(top_similar_composites, f)\n",
        "\n",
        "# Function to print top similar items\n",
        "def print_top_similar_items(title, items):\n",
        "    print(f\"Top 3 similar {title}:\")\n",
        "    for i, similarity_scores in enumerate(items):\n",
        "        print(f\"{title.capitalize()} {i + 1}:\")\n",
        "        print(f\"Image URL: {image_features['Image'][similarity_scores[0]]}\")\n",
        "        print(f\"Text Review: {image_features['Review Text'][similarity_scores[0]]}\")\n",
        "        print(f\"Image Score: {similarity_scores[1]}\")\n",
        "        print(f\"Review Text Score : {similarity_scores[2]}\")\n",
        "        print(f\"Composite Score: {similarity_scores[3]}\\n\")\n",
        "\n",
        "\n",
        "# Print top similar items\n",
        "print_top_similar_items(\"images\", top_similar_images)\n",
        "print_top_similar_items(\"reviews\", top_similar_reviews)\n",
        "print_top_similar_items(\"composite scores\", top_similar_composites)\n"
      ],
      "metadata": {
        "id": "j0MT3XoDZYqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da0f8a2-7bba-41f0-ddb1-0c72012c730a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter image URL (or press Enter to finish): https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
            "REVIEW: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
            "Top 3 similar images:\n",
            "Images 1:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
            "Text Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
            "Image Score: [0.94093215]\n",
            "Review Text Score : 0.9998998814927471\n",
            "Composite Score: [0.970416]\n",
            "\n",
            "Images 2:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg\n",
            "Text Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
            "Image Score: [0.78381854]\n",
            "Review Text Score : 0.10405063503843706\n",
            "Composite Score: [0.4439346]\n",
            "\n",
            "Images 3:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/71dCrR30OvL._SY88.jpg\n",
            "Text Review: Looking at these on a guitar when they don't have a single wrap on the post, I found it hard to believe the strings would even stay on but I have now installed two sets of these on two different guitars. One black set and the other is gold. The first guitar would not stay in tune so I tried these. They have been on for nearly a year now and they totally fixed my tuning problem. The second set I put on my brand new Gibson Les Paul only because the first ones worked so well. Super easy fast string changes with the auto trim feature, and no tuning issues at all. At 18:1 gear ratio they dial in a perfect tune and hold it. I prefer these over Grover or any other higher priced tuners available.\n",
            "I have added a few pictures. The new tuners covered the holes on my Les Paul so it looks great from the back. The Other is an Ibanez AR420. You can see the original mounting holes so I filled them with wood putty and will use model car paint to match to cover the spots.\n",
            "Image Score: [0.7573572]\n",
            "Review Text Score : 0.08147225536127889\n",
            "Composite Score: [0.41941473]\n",
            "\n",
            "Top 3 similar reviews:\n",
            "Reviews 1:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/61DvLcapd8L._SY88.jpg\n",
            "Text Review: I went from fender chrome non-locking to fender gold locking. It made my guitar look beautiful and play beautiful. I think locking tuners are the way to go. If you are new to locking tuners look on YouTube for instructions.\n",
            "Image Score: [0.33322233]\n",
            "Review Text Score : 0.2695022304554145\n",
            "Composite Score: [0.30136228]\n",
            "\n",
            "Reviews 2:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/811JMNm5LkL._SY88.jpg\n",
            "Text Review: I have both Teles and Strats, and they both rite in this case perfectly. The extra storage areas are great, and you don't get outside storage pockets on a hard case.  The case is very well made, durable, and light.  I think the price for this product is a very good value. I know this will suit my needs perfectly!\n",
            "Image Score: [0.54787797]\n",
            "Review Text Score : 0.1568772085818215\n",
            "Composite Score: [0.3523776]\n",
            "\n",
            "Reviews 3:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/61clqkZnKxL._SY88.jpg\n",
            "Text Review: Now all I have to do is install these on my Burswood Strat Copy. I know I'm gonna have to drill holes for the locating pins I may even have to drill the tuner holes also. Look forward to getting those crappy, loose, un-smooth, original hardware tuners off this thing.\n",
            "\n",
            "July 20, 2012:\n",
            "  Just installed these Fender locking tuners on my Strat. Didn't have to drill the tuner holes, they were the perfect size. Placed all the tuners in the headstock lined them up with a straight edge and just pressed each tuner very firmly into their hole (with the locking part screwed in tight so as not to damage them) and  very lightly and carefully tapped them with a small rubber mallet to mark my drill points. Then with a small magnifying glass and using a punch I made a center punch mark (just using pressure by hand with the punch since wood is soft) on each indent that the tuner made. Set my drill depth with a piece of tape on the drill and was done in less than 15 minutes.\n",
            "Image Score: [0.6117789]\n",
            "Review Text Score : 0.13866991664789857\n",
            "Composite Score: [0.3752244]\n",
            "\n",
            "Top 3 similar composite scores:\n",
            "Composite scores 1:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/711XbxFTQpL._SY88.jpg\n",
            "Text Review: There is not any noise during use, and not heating with 8 pedals powered on running for 5 hours, the voltage of each channel keeps the same. Stable power supplying. With 7 X 9V 100mA +1 X 9V 500mA+1 X 12V 100mA+1 X 18V 100mA can work for at least 10 pedals easily. 10X50 cm cables are long enough to reach any part of the pedal board. Small size, and will not taking up too much pace. The Polarity Reversal Cable can help you cope with different pedals. Stable output and cost-effective. Provides 10 power outputs, it is able to work for a standard pedalboard. But the price is not so expensive as the Voodoo Lab or T-Rex, etc, you can easily have a power supply to work for your own pedals. I am quite satisfied with the price and stability overall it is very worth having. NO HUM! EXCELLENT POWER! GET ONE!\n",
            "Image Score: [0.7413633]\n",
            "Review Text Score : 0.06070710146135451\n",
            "Composite Score: [0.4010352]\n",
            "\n",
            "Composite scores 2:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/71b2uBEDcAL._SY88.jpg\n",
            "Text Review: These wall hangers are superb. I don't understand why others of inferior quality cots twice as much. These are made out of solid wood (looks like Pinewood) glazed over with weather protective urethane. The great thing about this is that it supports all sizes of guitars, from small Ukuleles to full size. That is due to the rubber rings which they include to avoid smaller necks falling out. Another very nice thing is that the arm is screwed into a wood nut, which makes it super secured while allowing for movement in case you have odd neck shapes (like fender strats) which require a little angle.\n",
            "\n",
            "I have several of these and would for sure recommend it over any other one available due to the price point and the quality of the materials.\n",
            "Image Score: [0.73394907]\n",
            "Review Text Score : 0.054347498861322764\n",
            "Composite Score: [0.3941483]\n",
            "\n",
            "Composite scores 3:\n",
            "Image URL: https://images-na.ssl-images-amazon.com/images/I/71KBeByN-9L._SY88.jpg\n",
            "Text Review: Hold tune ok, but string clamps loosen while tuning.\n",
            "\n",
            "Update: Theyve been better as time goes on.\n",
            "Image Score: [0.7297663]\n",
            "Review Text Score : 0.054574881687271454\n",
            "Composite Score: [0.3921706]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}